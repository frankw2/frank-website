 Raskar UIST 2015 Keynote Transcript

(Feel free to add comments, add links and details to this raw transcript)
Slides are here: https://www.dropbox.com/s/5in5utdqo2lzg3z/raskarUISTnov2015v2.pdf?dl=0


Ramesh:                     
We live in a very confusing world. The largest taxi services company doesn't own any taxis. And the largest hospitality company in terms of market cap doesn't own any hotels. The largest media company doesn't own any media. And as we think about the next billions of people who are becoming newly digital, the solutions for them are also going to be very unique. So for me, what's exciting to see how this community at rest can think about solving problems for the next billions who are looking for not just solutions in white collar jobs or offices, but in many areas. In health, in education, in transport, in agriculture, in food, in hospitality, and even for daily wages, and farmers and agriculture.

And a lot of these areas were considered kind of separated from the computing world, because we didn't have a digital interface. And some of you might think that, hey the upcoming solutions are just a bunch of apps. I know, in San Francisco. But, instead of apps, let's think about DAPS, digital applications for physical services. Or DOPS, if you want to make it broader. Digital opportunities for physical services. And with DOPS and DAPS, now we have an opportunity in this community to really go impact the physical world in areas where simply we couldn't do it before.

Except. Now, and this transformation, this newly digital lifestyle that we all have is creating some hilarious situations. So I was just dropping my rental car after [inaudible][10:22], and I noticed this sign at Avis counter. And at the top right it says, "Have you forgotten your tapes and CD's?" When is the last time you carried tapes and CD's? And I chuckled, then I started looking at other parts of this, of this poster. And I say, "What are some other things that could disappear over the next few years?" Which one is next, that's going to disappear, you think?

Male 2:                       Camera.

Ramesh:                     The camera has almost disappeared, right.

Male 3:                       The wallet.

Ramesh:                     The wallet is almost gone. Keys. There is nothing special about keys, we should be able to eclipse that very soon. What about the glasses? We could put a prescription currently into our displays. But maybe the cellphone itself will disappear. With variables and invariables and digestibles. So, when we look at these solutions, I think it's clear that our interface to the digital world is going to be unique. So let's talk about the eye conditions. I like to start with.

Here I am, getting an eye test. For cataracts. You know, a device that really hasn't changed for the last 30 years. And the interface is the same. To get a retinal scan, this is a quarter million dollar device. To look at the back of your eye. But check out the user interface. You know, the nurse is going to shove my head, hopefully hard enough so that she gets a good picture. But the best part is, if she takes a picture and she does not get a good picture, you know what she does? She's going to use my head as her mouse.

And to get prescription for eyeglasses, [inaudible][12:23]. Now, it's kind of funny how horrible interfaces these are. At the same time, there are billions out there who need services, but don't get them because of lack of good interfaces. So it's clearly, it's clear that when we talk about interception of these emerging interfaces and emerging worlds, we can not think leniently. And we cannot just scale away, of we having scale. So my group did some work about five years ago with a device called Inetra, which is a snap-on like this that goes on top of a phone.

You look through it, click on a few buttons. When you're done, align some lines, and it gives you a prescription for your eyeglasses. Can scan for nearsightedness, farsightedness, and astigmatism. It can also scan for cataracts, this cloudiness in your eyes. Now the traditional solution for this involves shining lasers into your eye and using extremely high quality and highly sensitive image sensors. But you know, we have something magical in our pocket. When you have screens that have 300 plus dots per inch. They start calling them retina display.

But the quality is real enough that you can actually use the retina display to measure the eye stuff. Now, a traditional device, as I said, is called [inaudible][13:49] sensor. It shines a laser spot at the back of the eye. Is a laser pointer here? I'm going to shine it in the back of your eye. If anybody has one? Alright. Digital interface. You don't get that with physical devices. And the beacon that's clear in the back of your eye. Image light, image reference. And the sensor in this traditional, fractures as they call them, has micro lenses.

Hey Karen. I'm not on the panther scheme. At the back of the eye, it clears its energy trend, and it has this micro areas. If all the rays coming out of the lens are parallel, because it's exactly one focal length. You see this nice spot at the center of each of the micro lens. And that's when you have a normal eye. If you have nearsightedness, farsightedness, astigmatism, then those rays will not be parallel, and you'll see small displacement on each of those micro lenses. So this image sensor, and the displacement of those dots. You know, indicates a change in wave front.

And you can just do, you know, integrate the local slope just to reference integration to color the aberration in your eye. This is a great solution, has been around for about 30 years. How can they replace lasers and sensors with human interface? So here's an idea. I think the cellphone display. Same micro lenses, just inverse of what you just saw. And if you show the dots at the center of the smart lenses and you have a normal eye, you'll see a nice spot. So again, on the screen, this uniform area of dots. You see, you perceive a single dot.

But if you have an aberrated eye, then the same dots at the center of the shown at uniform creed, will actually create a jumble of dots. Right? So using the interface on the screen, you can simply displace those dots intentionally so that the rays are purely sorted, and then you get a single dot. This is a trick that you can use to display these 2 dots, which is 50 degrees of freedom to measure your reflective level. Now of course there's going to be an annoying interface if you have to do 50 degrees of freedom.

To compute just 3 values. Your spherical error, your cylindrical error, and your axis. So again, you can create a novel interface that allows you to navigate through this conventional space, but only quickly of each of those three parameters that you care about. So again, as far a complicated optics and sensing problem into an interface problem. Right? The snap on eyepiece costs next to nothing. It's plastic lenses and some prisms. And all inclusions is an interface and a software.

And this is kind of a thermometer for your eye. Because children don't go to school because they don't see very well. 2 billion people worldwide who need to wear glasses but don't wear them. Um, and there are labor who cannot do their job because they're not able to function. So it's just a problem of removing blurred vision. But it's a huge socioeconomic draws. And sometimes you just wanted to watch [inaudible][17:35] kick that goal in high definition. So we found this out as a venture.

And you're looking at all kinds of various clever solutions for interface. That can allow us to dramatically simplify entries of cost of complicated solutions. And more recently we started packing an even bigger challenge, which is taking a picture of the back of the eye. Remember the mouse? My head as a mouse? Well, what if we changed the problem? The biggest challenge in looking at the back of your eye, retina, is not imaging, but it's actually the alignment. Because the corneal reflection from your cornea.

You know, the very tiny pupil that you have through two or three limiters. Through all the alignment becomes very very challenging. Typically this has been solved, as I said, by just making sure the eyepiece is well aligned with the instrument. So we turn the problem around in a word mostly on paper. And we create visual cues inside this device. And we call it the eye selfy, because in a selfy you know the picture is being taken because you can see yourself. So instead of creating a camera that you could become an expert in aligning and taking a picture with all the feedback.

The simple user feedback, you know, solves their problem. So the thing is with eye selfy, it's not just a 2-d image, it's actually a 4-d image. A light for image. And as long as you're aligned, if you can see the line pattern, the camera can see you. And using these mechanics, it will see the back of the eye. So, taking these ideas of simplifying interfaces, I and my team had dozens of ideas how can we solve, you know, solutions for the blind, solutions for the visually challenged.

You know, even superhuman vision. But then I realized, if we do this one project at a time. You know, through masters, PhD's, and students. It could take us 15, 20 years to get all those ideas out. Can we do something about that? So this is the lab at MIT, and everybody has a 3-d printer. A lot of resources. You know, everybody's enjoying. But, you know the next 5 billion people, they may want to target, fortunately unfortunately, are not in Boston, are not near where you live. And so to solve that we need to go out there and understand about what these challenges are, what can be done from these communities.

And can solve problems that we're not even thinking about. So we launched an initiative called REDEx, rethinking, engineering, design, and execution. And our idea is to engage local innovators in finding the right products to work on, and not solve them in labs, but treat the whole world as a lab. So in this particular case we wanted to target solutions for the blind, for visually challenged, and critics who were working on vision. Let me show you a quick video... Hold on, let me get the audio right... Is the audio connected?

[music][21:24]

Ramesh:                     And so this is not a competition or a hackathon or some kind of outreach program. This is highly creative exploration and research. Two classes I teach to collaborators we have. We spend a lot of time thinking about what instruments to work on. We engage innovators all over the world. Find out what the right parameters do, like constraints we need to handle. We learn a lot from the challenges all there, and then we work together to solve it. And so then we started moving beyond just the eye conditions, and started looking at oral health. Or looking at sleep disorders.

Which is a huge challenge worldwide. Or new interfaces for looking at ear infections. Or also hearing tests. Now, these are all built with technologies that we use all the time. You know, electronics, computer vision, machine learning, signal processing. Uh, and all kinds of interface ideas. But we have to repurpose that in unique ways. Now, what's interesting when it comes to health today is that you know, in old times we make the doctors really experts. And they take, you know they go to medical school.

They get a lot of training, then you realize there's a limit to what a doctor can do, so we started making the instruments smarter, and they're amazing. But I would say the most underutilized source in a health care setting is the patient himself or herself. And if we as subjects can use really complex technologies, why are we being treated like vegetables. When we go and get health solutions. I think that's something we can do, if we as subjects are actually given a smart interface.

So by using this one simple idea of empowering the end user, we can come up with a whole bunch of solutions going forward. And as somebody has told us, you know, as good as AI is, IA can beat AI. Intelligence amplification can beat artificial intelligence. And we can use that in so many ways. And using this philosophy in our group we have built a whole range of solutions. You know, new stethoscopes, new [inaudible][25:43]scopes, new ways to measure infections, new ways to look at oral health, new types of ECG and EMG solutions. All using this basic device that we can empower the end user.

Now when I talk about emerging worlds, let's make sure we're talking about the right things. I think some time ago, emerging worlds meant developing countries. But we maybe mean something more than that. I think some time ago our efforts were about how can we save lives. You know, how can we distribute vaccines, how can we distribute [phonetic][26:24]mosqutonex. And then in the last 10 or 15 years, the position changed. And especially companies started thinking about. "How can we exploit emerging worlds as emerging markets?" But now we're in a very unique world where the business of new digital citizens.

We can truly co innovate with partners on the ground. And this is really a new model for us. So what are some things we're doing in our group to tackle this large problems that could be solved through technology and technical interface? Uh, how about a CAT scan machine that can fit in a rickshaw? Now it's said there are two billion people that need prescription glasses, but don't wear them. How can we modify our screens so that your prescription can be dialed in into your displays?

So the rest of the world will still look blurred if you take your prescription and put it on your screen, but the screen itself would look sharp. Um, and then any community setting. Schools and sharing professors would allow us to display a high quality information. And the basic idea behind that is to again convert a 2-d display into a light film display. You may have heard about effectively using kind of a unsharp masking on 2-d images, so that even the prescription, even the blur.

You would pre correct the 2-d image. It turns out, because of the nulls in the special frequencies, you would not be able to invert, it would be a ill conditioned problem. But in 4-d raised space, that inversion is actually very stable. So pre correcting the light frame, and displaying it for an individual with blurred vision is actually possible. And once you create a lightful display on the screen, which has to be sufficiently high resolution, you can dial in your prescription so the same display can be used for folks with varying individual prescription.

Now, how about reading a book without opening it page by page. In emerging words, in ancient words, there is a lot of archives. Beautiful information that's hidden away from us, because they simple cannot open the scrolls or books or artifacts. How would we do that? Using a different electromagnet spectrum, in this case using terahertz, and terahertz are at 300 microns to a millimeter in wavelength. But the pages of a book are about 20 micrometers thick. So, you know, a huge discrepancy between the wavelength and the thickness of the pages. So we're going to create new techniques in the lab that allow us to see the images.

I mean, the paper, the ink, and the air is nearly transparent in terms of terahertz. At the same time, the refractive index changes between the paper and the ink, is what allows us to get enough reflections off of those pages. But that difference is only about 4%. So we can start looking at these different texts on different pages. And also deal with occlusions as we are looking at it from above. Here we have nine pages with text on it. We could use a terahertz sensor to do interferometer measurements and compute raw data, the hypercube.

And then we're going to reshape this hypercube and start looking at the signals. Now, terahertz are unique. They're not like light, the images are not just intensity images. But these are fields, retro fields. And they might actually get inversions at the paper interface. And through all that, you can see that we have nine pages. You can see the V-shaped hypercubes, and we can start the OCR, and also take care of occlusions by building the physical model for heat ray removal of interference from dot layers.

Now, so far we can only look at about nine pages. We cannot read through a whole book, because we have to optimize the power, optimize the algorithms. But just image abilities to see through material that you simply cannot open. And we can take these ideas beyond imaging, beyond electromagnet spectrum. Also into electrical interfaces. Imagine an interface that allows you to do an effortless identification. In this case, through bio rhythms. So, without a password, without literacy to enter text, and without very cumbersome interfaces.

Now there's a demo you'll see in the evening by [inaudible][31:55] at Google. And how could you go beyond that and create interfaces to look at people in hazardous conditions, or in some cases to allow you to connect, just connect through walls. In this case we can use different frequencies, and once artists project you have seen. You can use other frequencies. Collaborators at MIT [inaudible][32:33] also friends at UW have been playing with Ardiv for quite some time. Now we can use radio frequencies and effectively create a camera, a microwave camera that can create 3-d dynamic images.

So here we have a mannequin behind a wall. And here you can see timed images with picosecond resolution. And we hope this will lead to new interfaces. But also lead to new opportunities in emerging worlds. And on the other demo that [inaudible][33:27] is showing in the evening, of using radio frequency signals for through wall interfaces. What are some other challenges in emerging worlds? Can you look at, can you detect cancer by detecting circling tumor cells through your body? And one of the methods to do that is through fluorescence lifetime imaging.

But can we create a physical interface that looks much like a blood pressure cuff and measure the fluorescence lifetime of tag cells. Fluoroform tag cells. And as know benign tumor versus malignant tumor, can change the lifetime of the tag fluoroform, and we can measure that. Or can we create completely new interfaces for oral health that use your toothbrush as an input device, as a sensing device. Now, as I said earlier, we live in a world where we get digital services from companies who don't own any of the physical assets.

And moving forward, when will we get health solutions without hospitals. We will learn without schools, we will grow food without depending on farms. Indoor farming. And we'll transact in yet a non currency. No physical currencies. So the digital opportunities for physical systems are huge. The way it has been solved by large organizations in the rich world was, let's develop the technology here and throw it over the wall in the emerging world, and see if it works. And that didn't go very far, and so some of them said "hey, let's open labs, and let's send X parts to Sao Paolo and Bangalore and Beijing." To see, maybe by aiding researchers, people will understand and innovate in interesting ways.

And Akira Toriyama in his book, Geek Essay, describes this problem of love amplification. Where technology in most cases simply amplifies what's already in the crowd. If things are getting better, they're getting better faster. Where if things are not going well, the technology on its own is not going to change it. So we need a third model, of a check out together model. Which is thinking about, to solve those digital problems for physical systems, how can you work together and work with hundreds and hundreds of innovators, inventors, practitioners together.

Now this is very very complex. As a computing community, also seems very daunting for us, of how we will achieve. So recently, we took a huge leap and said, let's try to look at not just health solutions and certain imaging solutions, but look at a collection of problems that we can target in an integrated place. And that's [phonetic][37:06]Komamila is 30 million people show up for 30 days for a pilgrimage. Has anybody heard of Komamila here? Yeah, so it's like Hodge, but it's like Hodge times 20, okay? The complexity is mind-blowing.

And it's a really beautiful, very colorful festival. Very colorful, very beautiful, very safe. I hope you'll get to experience yourself. And, you know over the years the festival has been going on for we don't even know, thousands of years. And there are always challenges, and they have been usually tackled through large police and paramedics and huge top down infrastructure. But then we have something interesting we have to do challenges in food and health and housing and transactions and so on.

So over the last two years, our team at MIT in collaboration with a lot of large companies and foundations have been making trips. Once roughly three months. To look at, what are some of the interesting opportunities, digital opportunities to tackle these challenges in coordination with the local government, local universities, and local businesses. And so we look at sensing technologies to count the crowd. We're looking at food distribution, food logistics. Various ways to steer the crowd using cellphone tower data. We are building housing, pop up housing.

Food distribution that reaches networks of places you cannot reach. And it has been just fascinating. I encourage you to go to our website to see all the efforts that have gone in. And anyone, everyone is welcome to come to participate. And the next camp is in just about a couple of months in very close to Bombay, end of January. So if anyone wants to come and see new opportunities for digital interfaces. So when it comes to big data or internet of things.

New abilities to citizens, it's obvious that we cannot think leniently and scale solutions that we already know. We have to leapfrog and think about solutions that are well beyond the out of sight and further out of our mind. So with some of the collaborators at MediaLab, we have [inaudible][40:27] as I said there's REDEx initiative, and we have centers popping up in many parts of the world. You may know some of these people. Sandy Pentland, Ken Larson, Ethan Zuckerman, Sousa Hidalgo, and Rial Dalbert. Also we have professor Ishi here from MediaLab, who constantly inspires me to do new things.

And as we approach, you know, the third yet, and your story of the West. In call of years. I think we have a very interesting milestone here. If you believe the hype out there. You know, the number of users coming online, the number of users using new digital solutions. It's mushrooming. We have our 30 million users every month coming online, who were not online before. So I hope that although we have made great progress in personal productivity or entertainment or organization.

Digitalized organizations at rest over the next 30 years. We will also look at additional professors to health, wealth, and well being. And being a faculty, it was always about publish or perish. And then it became easy enough to. And the reason it became publish or perish was the only way to get your ideas out was through publishing. Um, you submit your paper, your paper gets presented, and over the next five to ten years, somebody else will build on top of it, pick up your idea, and bring it in the real world.

And then the paradigm changed, and we started saying "demo or die". Because actually it's easy enough to prototype, because you build things and show it, not just right a paper about it. And somebody liked that, with the demo session I think there were around 30 plus amazing demos. But let's go one step beyond that. The same way what a few years ago became easy to demo live. It has become easier and easier to deploy. Through hardware technologies, through software platforms through cloud architectures, through worldwide networks.

And I hope, over the next 30 years, we'll have deploy sessions also at rest. By the way, the new mantra at MIT MediaLab is "deploy or do". Okay, how many of you remember this fantastic use of pencils? You know, for the most useful implementation of the pencil as an interface to music. And so we never know, you know, the things you build in this community can go out there as interface, and how they interface will be used out there. Now, we took a picture of this woman outside [phonetic][43:45]Hydrobod in India.

And she has converted a technology, which is a weighing scale into her business. But she's very proud of that. Look at the entrance to her business. She has a mat. She's very proud of it. So we never know through this micropreneurs and entrepreneurs how solutions and technologies and personal solutions we build in this community will have an impact out there in the world. And we're here to keep trying.

So to conclude, I think the world is our lab. There are billions of citizens that are hungry for new interfaces to physical solutions. There are industries and organizations that need complete openness in the emerging world. By co innovating with partners on the cloud. There are dozens and dozens of completely new search directions, and of course hundreds of new thesis topics. The world is our lab, so let's solve the billion dollar problems that will touch a billion lives. Thank you.

[applause][45:20]

Male 4:                       Hold on a second. Thank you very much for your keynote. So, I think now is a very good time just to devise what I missed. Fortunately I want to separate. So I've known you now for 15 years since we've worked. Actually we're former colleagues here at UL. And it's very different, very interesting career to watch. You know, taking projected into reality work that you did early on and then all through competition engineering. You know, and then looking at going global, going [inaudible][46:10].

And you're one of the TRL innovators, and one of the 20 Indian tech innovators that was awarded one of the first Indus tech innovator with your slim research fellowship operating on long shelf innovation. About 50 patents. And I think it's the trajectory that you actually have. The reason that you were invited here today. So what I'd like to do know is kind of ask the audience to see if they have questions for you. And there's a microphone over here, there's a microphone over there.

And however. If you just want to stay where you are, we're just going to send the microphones right over to you. So please just put up your hand if you have a question for Ramesh, and we'll get the microphones brought to you. That may be the easiest way to do it. We normally like queuing, but we're just going for questions from the audience. So put up your hand if you have a question.

Dale Wingdower:         Hi. [phonetic][47:15] Dale Wingdower from the University of Trunk. From early an intern beside your office at Berkeley. So I love the call to action when you start doing the coordinates and bust out the demos. But I'm worried about cost, and I'd love to have your thoughts on that. Not just on the cost of production, but also in the cost of maintenance and especially the cost of, if the technology is behind to move to the next project. How do we enable people to continue to use them and to build on them and incorporate them into their lives?

Ramesh:                     Yeah, very true. I mean, cost is just one aspect of deploy, but there are many of those. There is technology risk, there's adoption risk, there is acceptance of your solution. But as I said, I think a lot of those challenges are diminishing very very quickly. And deploy doesn't mean it has raised millions of dollars on kickstarter or is being used by USAID or someone has picked it up and has it in thousands of units.

It could be deployed with just five units or ten units. You know, if it's a software solution, it's being used 100 people. That's good enough, and you know, as much as we believe in user studies and user free bag, in constrained setting, I think now we have ability to do that in the wild. And that's what I mean by deploy. Deploy doesn't just mean millions of units. It could be five, ten, hundred to start with. And the cost, you know, that should be part of our community when we talk about research, we should be just citing grant proposals for salaries. But it should be part of the plan, that we're going to deploy. Five or ten or 100 of these.

Male 5:                       Hi, thank you for the talk Ramesh. I miss a bit buying product. Oh, is this right. Kintaro and I spoke. And you talked about the Komela festival, sorry if I'm mispronouncing that. And you mentioned that a lot of the infrastructure at Komela is done via centralized authorities. And sort of in the vein of Kintaro's book it seems like a lot of the infrastructure and services you're looking at, primarily people through sensors.

Having these distribution networks for food or for health services seem very centralized as well. I wonder if you've thought about that at all, because it seems very much in the vein of that. The technology really amplifies that structure.

Ramesh:                     Very true, and that's a great book, I encourage you to read it. I think it's changing, and I think Kintaro made a good point of, in my words, "here, there, and together". I think in our case, you know we have the government officials in real time with us. You know, I'm on the phone with the police commissioner. I'm there, my team is on the phone with the police commissioner. And, you know, with the health officials and so on.

It's a real time interaction. And also the initial, it's a very complex event with 30 million people, so there's only so much we can do. So we're very careful with where exactly we intervene. For example, for clustering we had six locations that tracked about a million people. Crossing the foot maps, and that created realtime data for prediction in which part of the [inaudible][51:05]. They would have, you know, more people coming. So they could predict that.

But we also used a completely new solution where we can see crowds on a desktop, on a dashboard moving through the cities. And that was done through cellphone tower data. So the cellphone towers as you know will give you the count of subscribers. And after deidentification, we can still use that data to map the density of the crowds. And so we would convince the city officials to give this data to us in real time. That's something that hasn't been done anywhere in the world. So we're able t5o use this cloud mapping and coordinate information back to the population.

But more importantly coordinate back to the control rooms and see where they're going. So you have to be very careful about where it goes. I should be very clear saying that this is just a beginning. It hasn't had a huge impact, because just the beginning. But I think there are ways to interface in this events like Komamila. And actually use a lot of search ideas, and to guard them.

Male 6:                       He asked the question I was going to ask. I got to spend time recently with the former CEO of FoxConn, and am now working with the hacking of the electronics supply chain in ChengZhen in China. And the answer to many questions that present itself is can the system built that we already have. Can it be broken apart, can it actually be hacked, and where can it be hacked. So we have a health care system in these countries that people will always say to them account hack them, account hack that.

IS actually to every single question like that, was that's what was not keen to say. So when you're saying you can't change the business, you can't change. People will always be using them, the way they were always used. He said, that's exactly what Nokia used to say to him as they were declining his domain and custom. This is the thing, these are very carefully constructed human systems for health care, and you're going in and you're actually empowering [inaudible][53:32]. To me, it looks like you're just hacking the health care system and empowering the individual.

And they are two conflicting world views. Is it safe to say you've "hacked" the system?

Ramesh:                     Yeah, I should be careful when using the word hack when it comes to patients. But, you know I think the transformation's already going on. You know, many of us use health and well being solutions that were unimaginable, you know, even five years ago. It's kind of impressive. If you look at the story of how diabetics started using insulin pumps. It took a long time for their transformation to happen to empower individual to use, to inject insulin into themselves.

You know, and the same thing with blood pressure cuffs. It took a long time for FDA to allow individuals to measure their own blood pressure through automated devices. But those are examples from some time ago. But now we're very quickly moving into a regime where the data is very important. And so the centers for measuring data, even from healthy individuals, is very very high. So we're slowly shifting from health care to health, and from health to wellness.

And this is a very interesting time for us in the computing community. In the interface community, because we can learn a lot by looking at the longitudinal data. You know, today's health solutions or health care solutions are infrequent point measurements that are extremely high quality. But now we're moving into a world where we have very frequent measurements, but they may be very low quality. All you may have is your heart rate and your skin conditions and your weight and your mental state and so on.

But this low quality but more frequent data streams can be very very powerful. So I think it can come in various levels. It can come in hacking physical hardware. It can also come through these data streams that can be exploited in very unique ways. It also goes back to, you know, we always hear about makers or diy. Or fabbing. And I think those are all fantastic movements. But instead of diy, do it yourself, why not do it for others? Or instead of makers that make it for their own enjoyment, why not make for others?

And those movements require very different ecosystems. I see everything here, and it has a lot of incidents in this field. And you know, let's move away from the hacking culture that's for fun and entertainment and personal satisfaction, and move towards some more systemic, systematic approach that comes embedded in this formal community of researchers.

Male 4:                       Unless we are no final questions, oh no, we've got something. Two questions.

Female 3:                   Ramesh, thank you so much for that talk, it's great. My question is not really specific to this talk. You've got this amazing trajectory in your career as [inaudible][56:51] was saying. We have a lot of patient students here in the room. I was wondering if you have advice for them as they make their way through this entrance career path. What would you tell them?

Ramesh:                     That's funny, because I had a slide for that. That's how professors do. So, I think they're going through a very destructive transformation of a classic PhD. As you can see, to have a real world impact is not just about publishing a demo. It's going to be more and more about deploy. To do that, we need to work in teams with rapid iterations. But you know a Master's or PhD has traditionally been a solo effort that's multi year.

And that is very very challenging. I think by the time you graduate, the stuff that you're taught is interesting to work on, maybe it has changed. Or maybe some legal technologies, some paralegal systems have made that obsolete. The way we really need to work is through teams and very swift iterations. So the same way the value of publication is going down nowadays, because people like to watch videos.

They like to watch even research videos. I think we have this very disruptive change that's coming up in PhD careers as well. I would be very surprised if, a few years down the line, a PhD student was judged based on their thesis. I think they'll be judged more on whether they were able to demo and deploy. And if they work in teams, and not if you create one solution, but cities of solutions that support your fundamental thesis.

Your fundamental argument. So I think it's going to be very challenging going forward as a traditional PhD student, and so I know my advice has a sort of bias built into it and a lot of attribute. But I would say work in teams, be out there. Don't do here or there, do it together.

Male 7:                       You said that effective partnerships are critical. How do we form effective partnerships?

Ramesh:                     The partnerships are very time consuming, because by the time you build the chemistry, by the time you identify the right problems and solutions. And that's why we started the REDEx platform. Rethinking, engineering, design, and execution. Where this communities, emerging communities. And they're not necessarily in developing countries.

They could be in any emerging areas. Are already anxious to find these problems, to solve these problems. And so we have been very fortunate to have constant connections with this, and it takes a lot of time. It takes two, three, four years before you actually have a meaningful initiative that's up and running. But going back to your question, I think we need those avenues. Not just universities and labs, but labs all over the world, or the world itself as a lab, so we can go forward.

All the REDEx centers and partnerships we have are open to any of you. And that's the only way that it's going to go forward. So I agree with your partnership for challenging. But I think that's going to be the new model. Those are the new places for the organization and deployment.

Male 7:                       We're there, okay. And I think as the individual recipients as members of the community has [inaudible][60:58] "demo or die". My take home is this possible void and disappear into the community. Because as competition goes into the fabric of life, it will become less today's news will be tomorrow's old newspapers. So it could disappear, if the fabric on everything else. So I just want to thank you very much on behalf of everyone here, again, for coming for your keynote. And we're going to go to a great session now.

Ramesh:                     Thank you.